---
title: "Exact and randomization tests"
author: "Steve Simon"
date: "Created 2023-05-12"
output:
  powerpoint_presentation:
    reference_doc: wide-screen-template.pptx
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../results", output_format = "all") })    
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(
  fig.width=6,
  fig.height=3,
  echo=FALSE,
  message=FALSE,
  warning=FALSE)
suppressMessages(
  suppressWarnings(
    library(tidyverse)
  )
)
```

### Where this fits

![](../images/where-this-fits.jpg)

<div class="notes">

Most of you should be familiar with the components and stages that The Analysis Factor uses to classify their talks. This talk is in Stage 3, Extensions of Linear Models. It covers a pretty broad swath, but might be considered as interpreting results. Perhaps validating results might be a second component.

</div>

### Goal

For you to have a good understanding of:

+ what randomization and exact tests are,
+ when it is appropriate to use them,
+ the steps to implement them.

The goal is not to cover every possible application of the randomization and exact tests.

<div class="notes">

I want to start with an appreciation of when you might consider using exact or randomization tests. These tests are very general, and I will try to show a variety of applications. If you want to implement these tests yourself, it is not too difficult for simpler hypotheses. For more complex hypotheses, you need some basic programming skills. It is not too difficult, if you know how to use loops inside SAS, R, or Stata.

It's not possible to cover every possible application of exact and randomization tests today. I hope just to get you a bit more comfortable with the methodology in general.

</div>


### Outline of topics

+ Historical origins of Fisher's Exact Test
+ Other exact tests
+ Randomization tests
+ Programming requirements
+ When should you use these tests

<div class="notes">

First, I will provide a historical overview, with an example derived in 1931, when Statistics was still in its infancy.
</div>

### Historical origins of Fisher's Exact Test

![Figure 1. Ronald A. Fisher](../images/Ronald_Aylmer_Fisher_1952.jpg)

<div class="notes">

Let's start with a historical overview.

</div>

### The lady tasting tea, tea plus milk

![Figure 2. Tea with milk added](../images/tea-plus-milk.png)

<div class="notes">



</div>

### Milk plus tea, can you tell the difference?

![Figure 3. Milk with tea added](../images/milk-plus-tea.png)

<div class="notes">



</div>

### The experiment to test the claim

![Figure 4. A randomized experiment](../images/tea-experiment.png)

<div class="notes">



</div>

### The result of the experiment

![Figure 5. Result of the randomized experiment](../images/tea-result.png)

<div class="notes">



</div>

### How likely is this result?

If the lady had no ability to tell whether the milk was added first and was effectively picking at random, the probability would be

+ $\frac{4}{8} \times \frac{3}{7}  \times \frac{2}{6} \times \frac{1}{5} = \frac{1}{70}$
  + Note: the probability is NOT $\left(\frac{1}{2}\right)^4$

<div class="notes">



</div>

### Break #1

What have you learned?
+ Simple application of Fisher's Exact test

What is coming next?
+ The hypergeometric distribution

### An alternate result

![Figure 6. An alternate result with one miss](../images/tea-result-2.png)

### How likely is three correct results?

$\frac{4}{8} \times \frac{4}{7}  \times \frac{3}{6} \times \frac{2}{5} + \frac{4}{8} \times \frac{4}{7}  \times \frac{3}{6} \times \frac{2}{5} +...$

Too messy! Use the hypergeometric distribution. Note: this is NOT a binomial distribution.

<div class="notes">



</div>

### Balls in an urn analogy

```{}

 |    W           |
 |       B        |
 |  B        B    |
 |          W     |
 |     W       B  |
 |        W       |
 __________________

```

<div class="notes">

Most of the discussion of these functions describe the hypergeometric distribution as an abstract problem of drawing balls from an urn. So think of the eight cups of tea as an urn with eight balls, four white and four black. The white balls represent correctly identifying the cup of tea as having the milk added first. The black balls represent mistakes in identification. So what is the probability of getting 3 white balls after drawing 4 balls without replacement?

</div>

### Use the hypergeometric distribution

+ w=# of white balls in the urn
+ b=# of black balls in the urn
+ n=w+b=# of balls total in the urn
+ d=# of balls drawn from the urn
+ x=# of drawn balls that are white
  + SAS: PDF('HYPER', x, n, w, d)
  + R: dhyper(x, w, b, d)
  + Stata: dis hypergeometricp(n, w, d, x)
  + SPSS: PDF.HYPER(x, n, w, d)
  
<div class="notes">

The functions to calculate hypergeometric probabilities vary from package to package. SAS uses a PDF function (short for Probability Density Function) with the 'HYPER' argument. R uses the dhyper function. Stata uses dis hypergeometricp. SPSS uses the PDF.HYPER function.

All of these packages arrange the numeric arguments differently.

Notice that R asks you to specify the number of white balls and the number of black balls. The other packages ask you to specify the number of white balls and the total number of balls, The order that you specify these values in is also inconsistent from package to package.

I show this to emphasize that if you plan to calculate hypergeometric probabilities, read the manual closely. It also helps to first do some simple calculations like an urn with 4 white and 4 black balls. Compare what the statistical package tells you with what you calculated by hand.

Fortunately, you don't need to resort to the hypergeometric probabilities.

</div>

### Break #2

What have you learned?
+ The hypergeometric distribution

What is coming next?
+ Using SPSS and Stata

### SPSS data for Fisher's Exact Test

![Figure 7. SPSS Dialog box](../images/spss-data.png)

### SPSS dialog boxes for Fisher's Exact Test (1/2)

![Figure 7. SPSS Dialog box](../images/spss-dialog-box-1.png)

### SPSS dialog boxes for Fisher's Exact Test (2/2)

![Figure 7. SPSS Dialog box](../images/spss-dialog-box-2.png)

### SPSS output for Fisher's Exact Test

![Figure 7. SPSS output box](../images/spss-output.png)

### Stata data for Fisher's Exact Test

![Figure 7. Stata data](../images/stata-data.png)

### Stata code and output for Fisher's Exact Test

```{}
. tabulate guess truth, exact

        Fisher's exact = 0.486
1-sided Fisher's exact = 0.243
```

### Break #3

What have you learned?
+ Using SPSS and Stata

What's coming next?
+ Details on the p-value computation

### Recall the definition of a p-value

p-value=P[sample results or more extreme| H0]

What does "more extreme" mean?

### Details on the p-value computation (2/5)

+ List all possible 2 by 2 tables
  + Restricted to common marginal totals
```{}
  ? ? | 4
  ? ? | 4
  ----+--
  4 4 | 8
```

### There are five tables with the same marginal totals

```{}
 4  0    3  1    2  2    1  3    0  4
 0  4    1  3    2  2    3  1    4  0
 
0.014   0.229   0.514   0.229   0.014
```

### Tables that are more extreme (1/2)

For a one-sided p-value

```{}
 4  0    3  1
 0  4    1  3
 
0.014 + 0.229

p-value = 0.243
```

### More extreme for a two-sided test (2/2)

For a two sided p-value

```{}
 4  0    3  1            1  3    0  4
 0  4    1  3            3  1    4  0
 
0.014 + 0.229     +     0.229 + 0.014

p-value = 0.486
```

### Break #4

What have you learned?
+ Details on the p-value computation

What's coming next?
+ More exact tests

### Fisher-Freeman-Halton test

+ Generalization of Fisher's Exact test
+ Tabulate all possible R by C tables
  + Fixed row and column totals
  
### R code for Fisher-Freeman-Halton test

```{}
> v <- c(4, 0, 0, 0, 4, 0, 0, 0, 4)
> m <- matrix(v, nrow=3)
> m
     [,1] [,2] [,3]
[1,]    4    0    0
[2,]    0    4    0
[3,]    0    0    4
```

### R output for Fisher-Freeman-Halton test in R

```{}
> fisher.test(m)

	Fisher's Exact Test for Count Data

data:  m
p-value = 0.0001732
alternative hypothesis: two.sided 
```

### Mann-Whitney U

Hypothetical data

```{}
T: 14, 23, 37
C: 12, 13, 15, 25
```

### Rank the data

```{}
T:  3,  5,  7
C:  1,  2,  4,  6
```

### Sum of the ranks

```{}
T = 15
C = 13
```

How likely is this result under the null hypothesis?

### List all possible ranking for T

```{}
1,2,3  1,2,4  1,2,5  1,2,6  1,2,7
1,3,4  1,3,5  1,3,6  1,3,7  1,4,5
1,4,6  1,4,7  1,5,6  1,5,7  1,6,7
2,3,4  2,3,5  2,3,6  2,3,7  2,4,5
2,4,6  2,4,7  2,5,6  2,5,7  2,6,7
3,4,5  3,4,6  3,4,7  3,5,6  3,5,7
3,6,7  4,5,6  4,5,7  4,6,7  5,6,7
```

### Select more extreme rankings

```{}




                            2,6,7
                            3,5,7
3,6,7  4,5,6  4,5,7  4,6,7  5,6,7
```

p-value = 7/35 = 0.20

### 

### More extreme rankings for a two-sided test

```{}
1,2,3  1,2,4  1,2,5  1,2,6
1,3,4  1,3,5

2,3,4
                            2,6,7
                            3,5,7
3,6,7  4,5,6  4,5,7  4,6,7  5,6,7
```

p-value = 14/35 = 0.40

### SAS code for Mann-Whitney test in SAS

```{}
proc npar1way wilcoxon;
  class grp;
  var x;
  exact wilcoxon;
run;
```

### SAS output for Mann-Whitney test (1/2)

![Figure x. SAS output](../images/sas-output-1.png)

### SAS output for Mann-Whitney test (2/2)

![Figure x. SAS output](../images/sas-output-2.png)

### Randomization test

![Figure x. 25 permutations](../images/randomization1.gif)

### Randomization test

![Figure x. 25 permutations](../images/randomization2.gif)

### Randomization test

![Figure x. 25 permutations](../images/randomization3.gif)

### When should you use exact or randomization tests?


### Conclusion
