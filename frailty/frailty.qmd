---
title: "Frailty models"
format: 
  pptx
---

## Fruit fly study, round 1 data

```         
  37         58         73
  40         59         75
  43         60         77
  44         61         79
  45         62         89
  47         68         94
  49         70         96
  54         71
  56         72
```

::: notes
Imagine an experiment where you monitor the survival time of 25 fruit flies. This is actually adapted from a real data set, but I have tweaked a few of the numbers to make things work out a bit easier.

The first fly dies on day 37 and the last fly dies on day 96.
:::

## Fruit fly study, round 1 probabilities

```         
  37  96%    58  60%    73  24%
  40  92%    59  56%    75  20%
  43  88%    60  52%    77  16%
  44  84%    61  48%    79  12%
  45  80%    62  44%    89   8%
  47  76%    68  40%    94   4%
  49  72%    70  36%    96   0%
  54  68%    71  32%  
  56  64%    72  38%
```

::: notes
If you want to estimate survival probabilities, just count the number of flies still alive on a given day and divide by 25. Each fly funeral leads to a 4% reduction in survival probability.
:::

## Fruit fly study, round 1 graph

![](fly-01.png)

::: notes
Here is what a graph of these probabilities would look like.
:::

## Fruit fly study, round 2 data

```         
  37         58         70+
  40         59         70+
  43         60         70+
  44         61         70+
  45         62         70+
  47         68         70+
  49         70+        70+
  54         70+
  56         70+
```

::: notes
Suppose that you ran that experiment, but on day 70, you left the cover off and 10 flies escaped. What a disaster, you think. The experiment is ruined.
:::

## Fruit fly study, round 2 probabilities

```         
  37  96%    58  60%    70+  ?
  40  92%    59  56%    70+  ?
  43  88%    60  52%    70+  ?
  44  84%    61  48%    70+  ?
  45  80%    62  44%    70+  ?
  47  76%    68  40%    70+  ?
  49  72%    70+  ?     70+  ?
  54  68%    70+  ?
  56  64%    70+  ?
```

::: notes
But hold on. You can still estimate survival probabilities up to 70 days. You can still estimate the median survival time (61 days). So all is not lost. You just lose survival times beyond 70 days.
:::

## Fruit fly study, round 2 graph

![](fly-02.png)

## Fruit fly study, round 3 data

```         
  37         58         70+
  40         59         75
  43         60         70+
  44         61         70+
  45         62         89
  47         68         70+
  49         70+        96
  54         71
  56         70+
```

::: notes
Suppose that you ran that experiment, but on day 70, you left the cover off and 6 of the 10 flies escaped. Now, you still have some data after 70 days. What do you do with it?
:::

## Fruit fly study, round 3 probabilities

```         
  37  96%    58  60%    70+
  40  92%    59  56%    75  20%
  43  88%    60  52%    70+
  44  84%    61  48%    70+
  45  80%    62  44%    89  10%
  47  76%    68  40%    70+
  49  72%    70+        96   0%
  54  68%    71  30%
  56  64%    70+
```

::: notes
You have to divvy up the remaining 40% of the survival probability among the 4 flies that remain. That means that each fly now carries 10% of the survival probability on their shoulders.
:::

## Fruit fly study, round 3 graph

![](fly-03.png)

::: notes
Here is what a graph of these probabilities would look like.
:::

## Life insurance example

```{r, echo=FALSE}
library(readr)
library(ggplot2)
df <- read_tsv("life-table.txt", col_names=FALSE)
names(df) <- c(
  "age",
  "h_male",
  "s_male",
  "life_expectancy_male",
  "h_female",
  "s_female",
  "life_expectancy_female"
)
df$density_male <-
  c(NA,
    (df$s_male[-120]-df$s_male[-1]))/100000
df <- df[1:110, ]
a1 <- 21
a2 <- 41
a3 <- 95
a4 <- 99
ga <- df$age >= a1 & df$age <= a2
gb <- df$age >= a3 & df$age <= a4
p1 <- sum(df$density_male[ga])
p2 <- sum(df$density_male[gb])
df1 <- c(0, df$density_male[ga], 0)
df2 <- c(0, df$density_male[gb], 0)
poly1 <- data.frame(x=c(a1, a1:a2, a2), y=df1)
poly2 <- data.frame(x=c(a3, a3:a4, a4), y=df2)
```

```{r, echo=FALSE}
ggplot(df, aes(age, density_male)) +
  theme_minimal() +
  geom_line()
```

::: notes
I found some data on mortality from the Social Security website and plotted an approximation to the probability density function. There is an unusual early peak in this function because the first year of your life is one of the most dangerous ones you will have to face.

Imagine yourself working in life insurance sales. You want to price your policies so that you only ask for low payments on the policy when the risk of death is low. So let's calculate some probabilities.
:::

## Probabilities for ages `r a1` through `r a2`

```{r}
ggplot(df, aes(age, density_male)) +
  geom_polygon(data=poly1, aes(x, y), fill="gray") +
  theme_minimal() +
  geom_line() +
  geom_text(
    aes(
      x=(a1+a2)/2, 
      y=0.001, 
      label=paste(round(100*p1, 1), "%")))
```

::: notes
The probability of a potential customer dying between the ages of `r a1` and `r a2` is `r p1`.
:::

## Probabilities for ages `r a3` through `r a4`

```{r}
ggplot(df, aes(age, density_male)) +
  geom_polygon(data=poly2, aes(x, y), fill="gray") +
  theme_minimal() +
  geom_line() +
  geom_text(
    aes(
      x=(a3+a4)/2, 
      y=0.001, 
      label=paste(round(100*p2, 1), "%")))
```

::: notes
The probability of a potential customer dying between the ages of `r a3` and `r a4` is about the same, `r p2`. So should you charge the same amount for an insurance policy for someone `r a1` years old and someone `r a3` years old?
:::

## Why are these probabilities not comparable?

-   Unequal time intervals
    -   Fix by computing a rate
-   Non-uniform probabilities over the interval
    -   Fix by looking at narrow interval
-   No adjustment for survivorship
    -   Fix by dividing by survival probabilty

::: notes
Obviously not. There are three things you need to fix first.

The most obvious flaw is the unequal time intervals, `r a2-a1` years for the first probability and `r a4-a3` years for the second probability.

You can fix this by computing a rate. You get the rate by dividing the probability by the width of the time interval.

The second flaw is that the probability changes over the interval, increasing in the first case and decreasing in the second case.

You can fix this by shrinking the width of the time interval.

The third flaw is a bit more subtle. The probability of dying between the ages of 95 and 99 are probabilities computed from the perspective of a newborn child. That probability is small not because the chances of dying are small at that age, but because so many have died before their `r a3`th birthday.

If you are in insurance sales, you do not sell policies to newborn infants. You sell to people who have survived to a certain age. No one rises from their grave on their `r a3`th birthday and asks for an insurance policy. First, because zombies aren't real, and second the zombie who died prior to year `r a3` would not be able to collect on an insurance policy that paid off for a death between `r a3` and `r a4`.

You can fix this by dividing by the survivor probability.
:::

## Hazard functions

-   $$h(t)=lim_{\Delta t \rightarrow 0}\frac{P[t \le T \le T+\Delta t]/\Delta t}{P[T \ge t]}$$

-   $$h(t)=\frac{f(t)}{S(t)}$$

    -   where $f$ is the density function, and
    -   $S$ is the survival function ($S(t)=1-F(t)$)

::: notes
The hazard function addresses all three of the concerns mentioned above. It computes a rate by dividing by $\Delta t$. It shrinks the interval but using a limit. And it adjusts for survivorship by dividing by the survivor probability.
:::

## Hazard function

```{r}
ggplot(df, aes(age, h_male)) +
  theme_minimal() +
  geom_line()
```

::: notes
This is what the hazard function for mortality data looks like.
:::

## Hazard function on a log scale

```{r}
ggplot(df, aes(age, h_male)) +
  theme_minimal() +
  geom_line() +
  scale_y_log10()
```

::: notes
The pattern becomes a bit clearer when you look at the hazard function on a log scale. The risk of death is high early in your life, but drops. There is a safe period during your pre-teen and early teen years, but then the risk rises because of an increase in deaths associated with things like driving, alcohol, and other drugs. Some of that fades as you mature but other risks increase because of the unavoidable aging of your body.
:::

## Cox publication

![](cox-paper.png)

::: notes
The hazard function provides the foundation for much work in survival analysis. This paper by Sir David Roxbee Cox introduced the proportional hazards regression model, also known as the Cox regression model. This paper has been cited over 28,000 times and represents the 24th most cited research paper in any field, according to a 2014 publication in Nature.
:::

## The Cox regression model

-   $h(t, X_i, \beta)=exp(X_i \beta)h_0(t)$
    -   The meaning of proportional hazards
        -   $\frac{h(t_i, X_i, \beta)}{h(t_i, X_j, \beta)}=exp(X_i-X_j) \beta$

::: notes
The Cox regression model states that the hazard function for a particular value of the independent variable is the exponential of X beta times a baseline hazard, h0. If you compare the hazard function at two levels of the covariate, Xi and Xj, the hazard function changes by a proportion equal to exp((Xi-Xj)\beta).
:::

## Estimation in the Cox model

-   Partial likelihood
    -   $\mathcal{l}_p(\beta)=\Pi_i \frac{exp(X_i \beta)}{\Sigma_{j \in R_i} exp(X_j \beta)}$
        -   R is all patients in the risk set
-   Log partial likelihood
    -   $\mathcal{L}_p(\beta)=\Sigma_i \Big(X_i \beta-ln(\Sigma_{j \in R_i} exp(X_j \beta)) \Big)$
-   $\hat\beta$ is the value that maximizes $\mathcal{L}_p(\beta)$

::: notes
A maximum likelihood approach to estimation does not work well because the hazard function burns up too many degrees of freedom. But you can compute a partial likelihood. The estimates from a Cox regression maximize this partial likelihood.

It is often easier to work on the log scale, and maximizing the log partial likelihood is equivalent.
:::

## Testing in the Cox model

-   Likelihood ratio test
    -   $2 (\mathcal{L}_p(\hat\beta)-\mathcal{L}_p(\hat\beta_{reduced}))$
-   Score test
    -   $\frac{\partial}{\partial\beta_j}\mathcal{L}_p(\hat\beta_{reduced})$
-   Wald test
    -   $I(\hat\beta) = -\frac{\partial^2}{\partial\beta^2}\mathcal{L}_p(\beta)$
    -   $se(\hat\beta_j) = \sqrt{I(\hat\beta)^{-1}_{jj}}$
    -   $T = \frac{\hat\beta_j}{se(\hat\beta_j)}$

::: notes
There are three tests that you can use for the Cox regression model. A partial likelihood ratio test compares the highest log partial likelihood (the log partial likelihood at $\hat\beta$ to the log partial likelihood at zero.

The score test looks at the first derivative of the partial likelihood evaluated at zero.

You can get a standard error for your estimates through the matrix of second partial derivatives of the log partial likelihood.

The formulas shown here area bit messy, but are very helpful when looking at various properties of the Cox regression model, such as residual analysis.
:::

## Violations of the independence assumption

-   Cox regression assumes that individual observations are independent. This may be problematic.
    -   Family/litter effects
    -   Multi-center trials
    -   Repeated measurements
        -   Left eye/right eye
        -   Infection
        -   Rehospitalization

::: notes
The Cox regression model assumes that the times are independent from one observation to another. This may be an issue in several settings, such as data from related family members, from multi-center trials, or from repeated measurments on the same subject.
:::

## Robust variance (sandwich estimator)

-   If observations are correlated
    -   Estimated coefficients, $\hat\beta$, are unbiased, but
    -   Estimated variance covariance matrix, $I(\hat\beta)^{-1}$ is biased.
-   Replace $I(\hat\beta)^{-1}$ with $I(\hat\beta)^{-1}(\hat L'\hat L)I(\hat\beta)^{-1}$
    -   $\hat L$ are score residuals, computed from the score statistic.
-   Comparable to the Generalized Estimating Equations (gee) model.

::: notes
When the observations are not independent, the regression coefficients, $\hat\beta$, are still unbiased, but the variances and covariances of these coefficients are messed up. This leads to confidence intervals that are way too wide or way too narrow. It also messes up the Type I error probabilities. Any sample size justifications based on an independence assumptions are invalid.

One approach to getting a proper estimate of the variances and covariances is to replace the inverse of the information matrix with a more complex estimate. This estimate uses residuals from the score statistic to adjust for the correlations induced by the family/litter effect, center effect, or repeated measurements.
:::

## Frailty model

-   Use double subscript for covariates
    -   i represents common family, litter, center, etc.
-   $h(t, X_ij, \beta)=exp(X_ij \beta)Z_ih_0(t)$
    -   $Z-i$ is usually a Gamma or lognormal distribution with a mean of 1.
-   Equivalent formulations
    -   $h(t, X_ij, \beta)=exp(X_ij \beta)exp(V_i)h_0(t)$
    -   $h(t, X_ij, \beta)=exp(X_ij \beta +V_i)h_0(t)$

::: notes
The frailty model adds an extra random term to the proportional hazards model that multiplies the hazard function by a variable that is constant across the cluster creating all the troublesome correlations.

The frailty term is usually a Gamma distribution or (less often) a lognormal distribution. The mean of either distribution is held to be 1.0. Clusters with a  term greater than 1 represent clusters that are more frail or more likely to experience the event. If the term is less than 1, that means the cluster is less frail or less likely to experience the event.

The frailty term is a latent variable, one that is not directly observable.
:::

## Rats data

```{r}
library(survival)
rats <- rats[rats$sex=="f", ]
print(rats[1:7, ], row.names=FALSE)
```

::: notes
Here are the first few rows of a data set looking at survival in rats. The researchers took either three males or three female rats from the same litter. One of the rats received an experimental treatment and the other two in the litter served as controls. I have reduced the dataset by looking only at female rats.
:::

## Analysis ignoring litter effect, Stata, 1 of 2

```         
stset time, failure(status)
stcox rx
```

::: notes
Here is the code in Stata to do a Cox regression. You suspect that the Cox model will not be valid, but it is still a useful first step.
:::

## Analysis ignoring litter effect, Stata, 2 of 2

![](stata-01.png)

::: notes
This is the output from Stata.
:::

## Analysis ignoring litter effect, SAS, 1 of 4

```         
proc phreg data=storage.rats;
  model time*status(0) = rx;
run;
```

::: notes
This is the code in SAS.
:::

## Analysis ignoring litter effect, SAS, 2 of 4

![](sas-01.png)

::: notes
SAS places its output in several tables. Here and on the next two slides are some of the key tables.
:::

## Analysis ignoring litter effect, SAS, 3 of 4

![](sas-02.png)


## Analysis ignoring litter effect, SAS, 4 of 4

![](sas-03.png)


## Analysis ignoring litter effect, R, 1 of 3

```{}
rats_surv <- Surv(rats$time, rats$status)
coxph(rats_surv ~ rx,
  data=rats)
```

::: notes
Here is the code in R.
:::

## Analysis ignoring litter effect, R, 2 of 3

::: {style="font-size: 50%;"}

```{r}
rats_surv <- Surv(rats$time, rats$status)
m1 <- coxph(rats_surv ~ rx, data=rats)
t1 <- capture.output(m1)
cat(t1[1:5], sep="\n")
```

::: notes
Here are the first few lines of output.
:::

## Analysis ignoring litter effect, R, 3 of 3

```{r}
cat(t1[6:8], sep="\n")
```

::: notes
Here is the rest of the output.
:::

## Analysis with robust variance (sandwich) estimate, Stata, 1 of 2

```         
stset time, failure(status)
stcox rx, vce(cluster litter)
```

::: notes
To fit the robust variance or sandwich estimate, include "vce" with the stcox procedure.
:::

## Analysis with robust variance (sandwich) estimate, Stata, 2 of 2

![](stata-02.png)

::: notes
Here is the output. Things have not changed that much, mainly because the litter effect is not very strong.
:::

## Analysis with robust variance (sandwich) estimate, SAS, 1 of 4

```         
proc phreg data=storage.rats covs(aggregate);
  model time*status(0) = rx;
  id litter;
run;
```

::: notes
The SAS code uses the "covs" keyword to get the robust variances.
:::

## Analysis with robust variance (sandwich) estimate, SAS, 2 of 4

![](sas-04.png)

::: notes
The key tables in SAS appear here and on the next two slides.
:::

## Analysis with robust variance (sandwich) estimate, SAS, 3 of 4

![](sas-05.png)

## Analysis with robust variance (sandwich) estimate, SAS, 4 of 4

![](sas-06.png)

## Analysis with robust variance (sandwich) estimate, R, 1 of 2

```         
rats_surv <- Surv(rats$time, rats$status)
coxph(rats_surv ~ rx + cluster(litter), data=rats)
```

::: notes
Use the "cluster" function in R to fit the robust variance model.
:::

## Analysis with robust variance (sandwich) estimate, R, 2 of 3

```{r}
rats_surv <- Surv(rats$time, rats$status)
m2 <- coxph(rats_surv ~ rx + cluster(litter), data=rats)
t2 <- capture.output(m2)
cat(t2[1:5], sep="\n")
```

::: notes
The output from R appears here and on the next slides.
:::

## Analysis with robust variance (sandwich) estimate, R, 3 of 3

```{r}
cat(t2[6:8], sep="\n")
```

## Analysis with frailty term, Stata, 1 of 2

```         
stset time, failure(status)
stcox rx, shared(litter)
```

::: notes
Stata refers to a "shared" frailty term, implying that the random term is shared by observations in the same cluster.
:::

## Analysis with frailty term, Stata, 2 of 2

![](stata-03.png)

::: notes
Here is the output in Stata.
:::

## Analysis with frailty term, SAS, 1 of 4

```         
proc phreg data=storage.rats;
  class litter;
  model time*status(0) = rx;
  id litter;
  random litter / dist=gamma;
run;
```

## Analysis with frailty term, SAS, 2 of 4

![](sas-07.png)

## Analysis with frailty term, SAS, 3 of 4

![](sas-08.png)

## Analysis with frailty term, SAS, 4 of 4

![](sas-09.png)

## Analysis with frailty term, R, 1 of 2

```         
rats_surv <- Surv(rats$time, rats$status)
coxph(rats_surv ~ rx + frailty(litter), data=rats)
```

## Analysis with frailty term, R, 2 of 3

```{r}
rats_surv <- Surv(rats$time, rats$status)
m3 <- coxph(rats_surv ~ rx + frailty(litter), data=rats)
t3 <- capture.output(m3)
cat(t3[1:6], sep="\n")
```

## Analysis with frailty term, R, 3 of 3

```{r}
cat(t3[8:12], sep="\n")
```

## Thiotepa data

```{r}
library(survival)
print(bladder2[1:7, ], row.names=FALSE)
```

## Plot of thiotepa data

```{r}
ggplot(bladder2, aes(id, stop)) +
  geom_segment(aes(x=0,xend=stop, y=id, yend=id)) +
  geom_text(aes(x=stop, y=id, label=event))
i <- 82
```

## Data for subject `r i`

```{r}
bi <- bladder2[bladder2$id==i,]
print(bi, row.names=FALSE)
```

## Re-setting the clock

```{r}
bj <- bi
bj$start[2:4] <- bj$start[2:4]-bj$stop[1:3]
bj$stop[2:4] <- bj$stop[2:4]-bj$stop[1:3]
print(bj, row.names=FALSE)
```

## Alternative analysis: Poisson regression

```{r}
bk <- bi[4, 1:4]
bk$event_count <- sum(bi$event)
bk$time_at_risk <- bi$stop[4]
print(bk, row.names=FALSE)
```

## Alternative analysis: stratification

-   Use a different baseline hazard for each cluster
    -   Only works with a few very large clusters
    
::: notes
One more alternative worth considering is stratification. Stratification fits a separate baseline hazard function for each cluster. This can use up a lot of degrees of freedom, so only consider this if you have a few very large clusters, such as in a multi-center clinical trial.
:::