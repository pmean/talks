---
title: "Frailty models"
format: 
  pptx
---

## Fruit fly study, round 1 data

```{}
  37         58         73
  40         59         75
  43         60         77
  44         61         79
  45         62         89
  47         68         94
  49         70         96
  54         71
  56         72
```

::: notes
Imagine an experiment where you monitor the survival time of 25 fruit flies. This is actually adapted from a real data set, but I have tweaked a few of the numbers to make things work out a bit easier.

The first fly dies on day 37 and the last fly dies on day 96.
:::

## Fruit fly study, round 1 probabilities

```{}
  37  96%    58  60%    73  24%
  40  92%    59  56%    75  20%
  43  88%    60  52%    77  16%
  44  84%    61  48%    79  12%
  45  80%    62  44%    89   8%
  47  76%    68  40%    94   4%
  49  72%    70  36%    96   0%
  54  68%    71  32%  
  56  64%    72  38%
```

::: notes
If you want to estimate survival probabilities, just count the number of flies still alive on a given day and divide by 25. Each fly funeral leads to a 4% reduction in survival probability.
:::



## Fruit fly study, round 1 graph

![](fly-01.png)

::: notes
Here is what a graph of these probabilities would look like.
:::

## Fruit fly study, round 2 data

```{}
  37         58         70+
  40         59         70+
  43         60         70+
  44         61         70+
  45         62         70+
  47         68         70+
  49         70+        70+
  54         70+
  56         70+
```

::: notes
Suppose that you ran that experiment, but on day 70, you left the cover off and 10 flies escaped. What a disaster, you think. The experiment is ruined.
:::

## Fruit fly study, round 2 probabilities

```{}
  37  96%    58  60%    70+  ?
  40  92%    59  56%    70+  ?
  43  88%    60  52%    70+  ?
  44  84%    61  48%    70+  ?
  45  80%    62  44%    70+  ?
  47  76%    68  40%    70+  ?
  49  72%    70+  ?     70+  ?
  54  68%    70+  ?
  56  64%    70+  ?
```

::: notes
But hold on. You can still estimate survival probabilities up to 70 days. You can still estimate the median survival time (61 days). So all is not lost. You just lose survival times beyond 70 days.
:::

## Fruit fly study, round 2 graph

![](fly-02.png)

## Fruit fly study, round 3 data

```{}
  37         58         70+
  40         59         75
  43         60         70+
  44         61         70+
  45         62         89
  47         68         70+
  49         70+        96
  54         71
  56         70+
```

::: notes
Suppose that you ran that experiment, but on day 70, you left the cover off and 6 of the 10 flies escaped. Now, you still have some data after 70 days. What do you do with it?
:::

## Fruit fly study, round 3 probabilities

```{}
  37  96%    58  60%    70+
  40  92%    59  56%    75  20%
  43  88%    60  52%    70+
  44  84%    61  48%    70+
  45  80%    62  44%    89  10%
  47  76%    68  40%    70+
  49  72%    70+        96   0%
  54  68%    71  30%
  56  64%    70+
```

::: notes
You have to divvy up the remaining 40% of the survival probability among the 4 flies that remain. That means that each fly now carries 10% of the survival probability on their shoulders.
:::

## Fruit fly study, round 3 graph

![](fly-03.png)

::: notes
Here is what a graph of these probabilities would look like.
:::

## Life insurance example

```{r}
library(readr)
library(ggplot2)
df <- read_tsv("life-table.txt", col_names=FALSE)
names(df) <- c(
  "age",
  "h_male",
  "s_male",
  "life_expectancy_male",
  "h_female",
  "s_female",
  "life_expectancy_female"
)
df$density_male <-
  c(NA,
    (df$s_male[-120]-df$s_male[-1]))/100000
df <- df[1:110, ]
a1 <- 21
a2 <- 41
a3 <- 95
a4 <- 99
ga <- df$age >= a1 & df$age <= a2
gb <- df$age >= a3 & df$age <= a4
p1 <- sum(df$density_male[ga])
p2 <- sum(df$density_male[gb])
df1 <- c(0, df$density_male[ga], 0)
df2 <- c(0, df$density_male[gb], 0)
poly1 <- data.frame(x=c(a1, a1:a2, a2), y=df1)
poly2 <- data.frame(x=c(a3, a3:a4, a4), y=df2)
```

```{r}
ggplot(df, aes(age, density_male)) +
  geom_line()
```

::: notes
I found some data on mortality from the Social Security website and plotted an approximation to the probability density function. There is an unusual early peak in this function because the first year of your life is one of the most dangerous ones you will have to face.

Imagine yourself working in life insurance sales. You want to price your policies so that you only ask for low payments on the policy when the risk of death is low. So let's calculate some probabilities.
:::

## Probabilities for ages `r a1` through `r a2`

```{r}
ggplot(df, aes(age, density_male)) +
  geom_polygon(data=poly1, aes(x, y), fill="white") +
  geom_line() +
  geom_text(
    aes(
      x=(a1+a2)/2, 
      y=0.001, 
      label=paste(round(100*p1, 1), "%")))
```

::: notes
The probability of a potential customer dying between the ages of `r a1` and `r a2` is `r p1`.
:::

## Probabilities for ages `r a3` through `r a4`

```{r}
ggplot(df, aes(age, density_male)) +
  geom_polygon(data=poly2, aes(x, y), fill="white") +
  geom_line() +
  geom_text(
    aes(
      x=(a3+a4)/2, 
      y=0.001, 
      label=paste(round(100*p2, 1), "%")))
```

::: notes
The probability of a potential customer dying between the ages of `r a3` and `r a4` is about the same, `r p2`. So should you charge the same amount for an insurance policy for someone `r a1` years old and someone `r a3` years old?
:::

## Why are these probabilities not comparable?

-   Unequal time intervals
    -   Fix by computing a rate
-   Non-uniform probabilities over the interval
    -   Fix by looking at narrow interval
-   No adjustment for survivorship
    -   Fix by dividing by survival probabilty

::: notes
Obviously not. There are three things you need to fix first.

The most obvious flaw is the unequal time intervals, `r a2-a1` years for the first probability and `r a4-a3` years for the second probability.

You can fix this by computing a rate. You get the rate by dividing the probability by the width of the time interval.

The second flaw is that the probability changes over the interval, increasing in the first case and decreasing in the second case. 

You can fix this by shrinking the width of the time interval.

The third flaw is a bit more subtle. The probability of dying between the ages of 95 and 99 are probabilities computed from the perspective of a newborn child. That probability is small not because the chances of dying are small at that age, but because so many have died before their `r a3`th birthday.

If you are in insurance sales, you do not sell policies to newborn infants. You sell to people who have survived to a certain age. No one rises from their grave on their `r a3`th birthday and asks for an insurance policy. First, because zombies aren't real, and second the zombie who died prior to year `r a3` would not be able to collect on an insurance policy that paid off for a death between `r a3` and `r a4`.

You can fix this by dividing by the survivor probability.
:::

## Hazard functions

-   $$h(t)=lim_{\Delta t \rightarrow 0}\frac{P[t \le T \le T+\Delta t]/\Delta t}{P[T \ge t]}$$

-   $$h(t)=\frac{f(t)}{S(t)}$$
    -   where $f$ is the density function, and 
    -   $S$ is the survival function ($S(t)=1-F(t)$)

::: notes
The hazard function addresses all three of the concerns mentioned above. It computes a rate by dividing by $\Delta t$. It shrinks the interval but using a limit. And it adjusts for survivorship by dividing by the survivor probability.
:::

## Hazard function

```{r}
ggplot(df, aes(age, h_male)) +
  geom_line()
```

::: notes
This is what the hazard function for mortality data looks like.
:::

## Hazard function on a log scale

```{r}
ggplot(df, aes(age, h_male)) +
  geom_line() +
  scale_y_log10()
```

::: notes
The pattern becomes a bit clearer when you look at the hazard function on a log scale. The risk of death is high early in your life, but drops. There is a safe period during your pre-teen and early teen years, but then the risk rises because of an increase in deaths associated with things like driving, alcohol, and other drugs. Some of that fades as you mature but other risks increase because of the unavoidable aging of your body.
:::

## Cox publication

![](cox-paper.png)

::: notes
The hazard function provides the foundation for much work in survival analysis. This paper by Sir David Roxbee Cox introduced the proportional hazards regression model, also known as the Cox regression model. This paper has been cited over 28,000 times and represents the 24th most cited research paper in any field, according to a 2014 publication in Nature.
:::

## The Cox regression model

-   $h(t, X_i, \beta)=exp(X_i \beta)h_0(t)$
    -   The meaning of proportional hazards
        -   $\frac{h(t_i, X_i, \beta)}{h(t_i, X_j, \beta)}=exp(X_i-X_j) \beta$
        
::: notes
The Cox regression model states that the hazard function for a particular value of the independent variable is the exponential of X beta times a baseline hazard, h0. If you compare the hazard function at two levels of the covariate, Xi and Xj, the hazard function changes by a proportion equal to exp((Xi-Xj)\beta).
:::

## Estimation in the Cox model

-   Partial likelihood
    -   $\mathcal{l}_p(\beta)=\Pi_i \frac{exp(X_i \beta)}{\Sigma_{j \in R_i} exp(X_j \beta)}$
        -    R is all patients in the risk set
-   Log partial likelihood
    -   $\mathcal{L}_p(\beta)=\Pi_i \Big(X_i \beta-ln(\Sigma_{j \in R_i} exp(X_j \beta)) \Big)$
-   $\hat\beta$ is the value that maximizes $\mathcal{L}_p(\beta)$    
    
::: notes
A maximum likelihood approach to estimation does not work well because the hazard function burns up too many degrees of freedom. But you can compute a partial likelihood. The estimates from a Cox regression maximize this partial likelihood. 

It is often easier to work on the log scale, and maximizing the log partial likelihood is equivalent.
:::

## Testing in the Cox model

-   Likelihood ratio test
    -   $2 (\mathcal{L}_p(\hat\beta)-\mathcal{L}_p(0))$
-   Score test
    -   $\frac{\partial}{\partial\beta_j}\mathcal{L}_p(0)$
-   Wald test
    -   $I(\beta) = -\frac{\partial^2}{\partial\beta^2}\mathcal{L}_p(\beta)$
    -   $se(\hat\beta) = \sqrt{I(\hat\beta)^{-1}_{jj}}$
    -   $T = \frac{\hat\beta}{se(\hat\beta)}$

::: notes
There are three tests that you can use for the Cox regression model. A partial likelihood ratio test compares the highest log partial likelihood (the log partial likelihood at $\hat\beta$ to the log partial likelihood at zero.

The score test looks at the first derivative of the partial likelihood evaluated at zero.

You can get a standard error for your estimates through the matrix of second partial derivatives of the log partial likelihood.

The formulas shown here area bit messy, but are very helpful when looking at various properties of the Cox regression model, such as residual analysis.
:::

## Robust variance (sandwich estimator)

-   The variance covariance matrix, $I(\hat\beta)^{-1}$ is biased.
-   Replace it with the sandwich estimate, $I(\hat\beta)^{-1}(\hat L'\hat L)I(\hat\beta)^{-1}$
    -   $\hat L$ are score residuals, computed from the score statistic.
-   Comparable to the Generalized Estimating Equations (gee) model.
    